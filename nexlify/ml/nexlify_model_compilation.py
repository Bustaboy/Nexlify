#!/usr/bin/env python3
"""
Nexlify Model Compilation

Automatic model compilation for 20-50% speedup:
- torch.compile() (PyTorch 2.0+) - 30-50% faster
- TorchScript JIT - 20-30% faster
- TensorRT (NVIDIA) - 2-5x faster inference
- ONNX Runtime - Cross-platform inference
- Automatic backend selection

ZERO OVERHEAD: Compilation happens once, then lightning fast
"""

import logging
import time
from typing import Optional, Dict, Any, Callable
from enum import Enum
from dataclasses import dataclass

logger = logging.getLogger(__name__)


class CompilationBackend(Enum):
    """Compilation backends"""
    NONE = "none"                     # No compilation
    TORCH_COMPILE = "torch_compile"   # PyTorch 2.0+ compile (best for training)
    TORCHSCRIPT = "torchscript"       # TorchScript JIT (good for inference)
    TENSORRT = "tensorrt"             # NVIDIA TensorRT (fastest inference)
    ONNX = "onnx"                     # ONNX Runtime (cross-platform)
    OPENVINO = "openvino"             # Intel OpenVINO


class CompilationMode(Enum):
    """Compilation modes"""
    DEFAULT = "default"               # Balanced
    REDUCE_OVERHEAD = "reduce-overhead"  # Optimize for many small ops
    MAX_AUTOTUNE = "max-autotune"    # Find best kernels (slow compile, fast runtime)
    INFERENCE = "inference"          # Optimize for inference only


@dataclass
class CompilationConfig:
    """Model compilation configuration"""
    backend: CompilationBackend
    mode: CompilationMode
    dynamic_shapes: bool
    fullgraph: bool  # Try to compile entire model (vs fallback)
    use_fp16: bool
    use_int8: bool


class ModelCompiler:
    """
    Automatic model compilation with backend selection

    Speedups:
    - torch.compile: 30-50% faster (PyTorch 2.0+)
    - TorchScript: 20-30% faster
    - TensorRT: 2-5x faster (NVIDIA)
    """

    def __init__(self):
        self.torch_version = None
        self.has_torch_compile = False
        self.has_tensorrt = False
        self.has_onnx = False

        # Check available backends
        self._check_backends()

        logger.info("âš¡ Model Compiler initialized")
        if self.has_torch_compile:
            logger.info("   âœ“ torch.compile available (PyTorch 2.0+)")
        if self.has_tensorrt:
            logger.info("   âœ“ TensorRT available")
        if self.has_onnx:
            logger.info("   âœ“ ONNX Runtime available")

    def _check_backends(self):
        """Check available compilation backends"""
        try:
            import torch

            self.torch_version = torch.__version__

            # Check for torch.compile (PyTorch 2.0+)
            if hasattr(torch, 'compile'):
                self.has_torch_compile = True

            # Check for TensorRT
            try:
                import torch_tensorrt
                self.has_tensorrt = True
            except ImportError:
                pass

            # Check for ONNX Runtime
            try:
                import onnxruntime
                self.has_onnx = True
            except ImportError:
                pass

        except ImportError:
            logger.warning("PyTorch not available")

    def compile(self,
                model,
                backend: Optional[CompilationBackend] = None,
                mode: CompilationMode = CompilationMode.DEFAULT,
                dynamic_shapes: bool = False,
                example_inputs: Optional[Any] = None) -> Any:
        """
        Compile model for faster execution

        Args:
            model: PyTorch model
            backend: Compilation backend (None = auto-select)
            mode: Compilation mode
            dynamic_shapes: Support dynamic input shapes
            example_inputs: Example inputs for tracing

        Returns:
            Compiled model (20-50% faster!)
        """
        if backend is None:
            backend = self._select_backend()

        logger.info(f"ðŸ”§ Compiling model with {backend.value}...")
        start_time = time.time()

        try:
            if backend == CompilationBackend.TORCH_COMPILE:
                compiled = self._compile_torch_compile(model, mode, dynamic_shapes)

            elif backend == CompilationBackend.TORCHSCRIPT:
                compiled = self._compile_torchscript(model, example_inputs)

            elif backend == CompilationBackend.TENSORRT:
                compiled = self._compile_tensorrt(model, example_inputs)

            elif backend == CompilationBackend.ONNX:
                compiled = self._compile_onnx(model, example_inputs)

            else:
                # No compilation
                logger.info("   No compilation - using eager mode")
                return model

            compile_time = time.time() - start_time
            logger.info(f"   âœ… Compilation complete ({compile_time:.1f}s)")
            logger.info(f"   Expected speedup: 20-50%")

            return compiled

        except Exception as e:
            logger.error(f"Compilation failed: {e}")
            logger.warning("Falling back to eager mode")
            return model

    def _select_backend(self) -> CompilationBackend:
        """Auto-select best compilation backend"""
        # Prefer torch.compile (PyTorch 2.0+) - best balance
        if self.has_torch_compile:
            return CompilationBackend.TORCH_COMPILE

        # Fallback to TorchScript
        return CompilationBackend.TORCHSCRIPT

    def _compile_torch_compile(self,
                               model,
                               mode: CompilationMode,
                               dynamic_shapes: bool) -> Any:
        """
        Compile with torch.compile (PyTorch 2.0+)

        Speedup: 30-50% for training, 40-60% for inference
        """
        import torch

        if not self.has_torch_compile:
            raise RuntimeError("torch.compile not available (need PyTorch 2.0+)")

        # Map our mode to torch mode
        if mode == CompilationMode.REDUCE_OVERHEAD:
            torch_mode = "reduce-overhead"
        elif mode == CompilationMode.MAX_AUTOTUNE:
            torch_mode = "max-autotune"
        else:
            torch_mode = "default"

        logger.info(f"   Using torch.compile (mode: {torch_mode})")

        # Compile
        compiled = torch.compile(
            model,
            mode=torch_mode,
            dynamic=dynamic_shapes,
            fullgraph=False,  # Allow fallback for unsupported ops
        )

        return compiled

    def _compile_torchscript(self, model, example_inputs: Optional[Any] = None) -> Any:
        """
        Compile with TorchScript JIT

        Speedup: 20-30% for inference
        """
        import torch

        logger.info("   Using TorchScript JIT")

        if example_inputs is not None:
            # Trace mode (faster, but requires example inputs)
            model.eval()
            compiled = torch.jit.trace(model, example_inputs)
            logger.info("   TorchScript: trace mode")
        else:
            # Script mode (slower, but doesn't need inputs)
            compiled = torch.jit.script(model)
            logger.info("   TorchScript: script mode")

        # Optimize
        compiled = torch.jit.optimize_for_inference(compiled)

        return compiled

    def _compile_tensorrt(self, model, example_inputs: Any) -> Any:
        """
        Compile with TensorRT (NVIDIA only)

        Speedup: 2-5x for inference
        """
        if not self.has_tensorrt:
            raise RuntimeError("TensorRT not available (pip install torch-tensorrt)")

        import torch
        import torch_tensorrt

        logger.info("   Using TensorRT (NVIDIA)")

        model.eval()

        # Compile
        compiled = torch_tensorrt.compile(
            model,
            inputs=[example_inputs],
            enabled_precisions={torch.float16},  # FP16 for speed
            workspace_size=1 << 30  # 1GB
        )

        return compiled

    def _compile_onnx(self, model, example_inputs: Any) -> Any:
        """
        Export to ONNX for cross-platform inference

        Speedup: 30-40% with ONNXRuntime
        """
        if not self.has_onnx:
            raise RuntimeError("ONNX Runtime not available (pip install onnxruntime)")

        import torch
        import onnx
        import onnxruntime as ort

        logger.info("   Exporting to ONNX")

        model.eval()

        # Export to ONNX
        onnx_path = "/tmp/model.onnx"

        torch.onnx.export(
            model,
            example_inputs,
            onnx_path,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
        )

        # Create ONNX Runtime session
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
        session = ort.InferenceSession(onnx_path, providers=providers)

        logger.info(f"   ONNX Runtime provider: {session.get_providers()[0]}")

        return session


class CompilationOptimizer:
    """
    Automatically applies compilation with profiling

    Compares compiled vs eager mode and chooses best
    """

    def __init__(self):
        self.compiler = ModelCompiler()

    def optimize_model(self,
                      model,
                      example_inputs,
                      num_warmup: int = 10,
                      num_profile: int = 100) -> Any:
        """
        Optimize model by profiling different compilation backends

        Args:
            model: PyTorch model
            example_inputs: Example inputs for profiling
            num_warmup: Warmup iterations
            num_profile: Profiling iterations

        Returns:
            Best compiled model
        """
        import torch

        logger.info("ðŸ” Profiling compilation backends...")

        results = {}

        # Benchmark eager mode
        logger.info("   Benchmarking eager mode...")
        eager_time = self._benchmark_model(model, example_inputs, num_warmup, num_profile)
        results['eager'] = eager_time
        logger.info(f"      Eager mode: {eager_time*1000:.2f}ms per iteration")

        # Try torch.compile
        if self.compiler.has_torch_compile:
            logger.info("   Benchmarking torch.compile...")
            try:
                compiled = self.compiler.compile(
                    model,
                    backend=CompilationBackend.TORCH_COMPILE,
                    example_inputs=example_inputs
                )
                compile_time = self._benchmark_model(compiled, example_inputs, num_warmup, num_profile)
                results['torch_compile'] = compile_time
                speedup = eager_time / compile_time
                logger.info(f"      torch.compile: {compile_time*1000:.2f}ms ({speedup:.2f}x speedup)")
            except Exception as e:
                logger.warning(f"      torch.compile failed: {e}")

        # Try TorchScript
        logger.info("   Benchmarking TorchScript...")
        try:
            compiled = self.compiler.compile(
                model,
                backend=CompilationBackend.TORCHSCRIPT,
                example_inputs=example_inputs
            )
            script_time = self._benchmark_model(compiled, example_inputs, num_warmup, num_profile)
            results['torchscript'] = script_time
            speedup = eager_time / script_time
            logger.info(f"      TorchScript: {script_time*1000:.2f}ms ({speedup:.2f}x speedup)")
        except Exception as e:
            logger.warning(f"      TorchScript failed: {e}")

        # Select best
        best_backend = min(results, key=results.get)
        best_time = results[best_backend]
        speedup = eager_time / best_time

        logger.info(f"\nâœ… Best backend: {best_backend} ({speedup:.2f}x speedup)")

        # Return compiled model with best backend
        if best_backend == 'torch_compile':
            return self.compiler.compile(model, CompilationBackend.TORCH_COMPILE, example_inputs=example_inputs)
        elif best_backend == 'torchscript':
            return self.compiler.compile(model, CompilationBackend.TORCHSCRIPT, example_inputs=example_inputs)
        else:
            return model

    def _benchmark_model(self, model, inputs, num_warmup: int, num_profile: int) -> float:
        """Benchmark model inference time"""
        import torch

        model.eval()

        # Warmup
        with torch.no_grad():
            for _ in range(num_warmup):
                _ = model(inputs)

        # Profile
        torch.cuda.synchronize() if torch.cuda.is_available() else None

        start_time = time.time()

        with torch.no_grad():
            for _ in range(num_profile):
                _ = model(inputs)

        torch.cuda.synchronize() if torch.cuda.is_available() else None

        total_time = time.time() - start_time
        avg_time = total_time / num_profile

        return avg_time


# Convenience functions
def compile_model(model,
                 backend: Optional[CompilationBackend] = None,
                 mode: CompilationMode = CompilationMode.DEFAULT,
                 example_inputs: Optional[Any] = None) -> Any:
    """
    Compile model for 20-50% speedup

    Args:
        model: PyTorch model
        backend: Compilation backend (None = auto)
        mode: Compilation mode
        example_inputs: Example inputs

    Returns:
        Compiled model
    """
    compiler = ModelCompiler()
    return compiler.compile(model, backend, mode, example_inputs=example_inputs)


def auto_optimize_model(model, example_inputs) -> Any:
    """
    Automatically find best compilation for model

    Args:
        model: PyTorch model
        example_inputs: Example inputs for profiling

    Returns:
        Optimized model (20-50% faster!)
    """
    optimizer = CompilationOptimizer()
    return optimizer.optimize_model(model, example_inputs)


# Export
__all__ = [
    'CompilationBackend',
    'CompilationMode',
    'CompilationConfig',
    'ModelCompiler',
    'CompilationOptimizer',
    'compile_model',
    'auto_optimize_model'
]
