#!/usr/bin/env python3
"""
Nexlify Automatic Quantization

Reduce model size by 4x with minimal accuracy loss:
- INT8 quantization (4x smaller, 2-4x faster)
- Dynamic quantization (automatic, zero effort)
- Static quantization (calibration dataset)
- Quantization-Aware Training (QAT)
- Per-channel quantization (better accuracy)

MASSIVE SAVINGS: 4x less memory, 2-4x faster inference
"""

import logging
import copy
from typing import Optional, Dict, Any, Callable, List
from enum import Enum
from dataclasses import dataclass

logger = logging.getLogger(__name__)


class QuantizationMethod(Enum):
    """Quantization methods"""

    NONE = "none"
    DYNAMIC = "dynamic"  # Easy: quantize at runtime
    STATIC = "static"  # Better: use calibration data
    QAT = "qat"  # Best: quantization-aware training
    FP16 = "fp16"  # Half precision (2x savings)


class QuantizationBackend(Enum):
    """Quantization backends"""

    PYTORCH = "pytorch"  # PyTorch native
    FBGEMM = "fbgemm"  # x86 CPU (Intel/AMD)
    QNNPACK = "qnnpack"  # ARM CPU (mobile)
    TENSORRT = "tensorrt"  # NVIDIA GPU


@dataclass
class QuantizationConfig:
    """Quantization configuration"""

    method: QuantizationMethod
    backend: QuantizationBackend
    per_channel: bool  # Better accuracy
    symmetric: bool  # Symmetric quantization
    reduce_range: bool  # For older CPUs
    calibration_batches: int  # For static quantization


class AutoQuantizer:
    """
    Automatic model quantization

    Reduces model size by 4x with minimal accuracy loss:
    - Dynamic: Zero effort, good speedup
    - Static: Needs calibration data, better speedup
    - QAT: Best accuracy, requires retraining
    """

    def __init__(self):
        self.torch_version = None
        self.available_backends = []

        self._check_backends()

        logger.info("ðŸ“¦ Auto Quantizer initialized")
        logger.info(
            f"   Available backends: {[b.value for b in self.available_backends]}"
        )

    def _check_backends(self):
        """Check available quantization backends"""
        try:
            import torch
            import torch.quantization

            self.torch_version = torch.__version__

            # FBGEMM for x86
            if (
                hasattr(torch.backends, "fbgemm")
                and torch.backends.fbgemm.is_available()
            ):
                self.available_backends.append(QuantizationBackend.FBGEMM)

            # QNNPACK for ARM
            if (
                hasattr(torch.backends, "qnnpack")
                and torch.backends.qnnpack.is_available()
            ):
                self.available_backends.append(QuantizationBackend.QNNPACK)

            # PyTorch native always available
            self.available_backends.append(QuantizationBackend.PYTORCH)

        except ImportError:
            logger.warning("PyTorch not available")

    def quantize(
        self,
        model,
        method: QuantizationMethod = QuantizationMethod.DYNAMIC,
        calibration_data: Optional[Any] = None,
        backend: Optional[QuantizationBackend] = None,
    ) -> Any:
        """
        Quantize model automatically

        Args:
            model: PyTorch model
            method: Quantization method
            calibration_data: Calibration dataset (for static quantization)
            backend: Quantization backend (None = auto-select)

        Returns:
            Quantized model (4x smaller, 2-4x faster!)
        """
        import torch

        if backend is None:
            backend = self._select_backend()

        logger.info(
            f"ðŸ“¦ Quantizing model (method: {method.value}, backend: {backend.value})..."
        )

        try:
            if method == QuantizationMethod.DYNAMIC:
                quantized = self._quantize_dynamic(model, backend)

            elif method == QuantizationMethod.STATIC:
                if calibration_data is None:
                    logger.warning(
                        "Static quantization needs calibration data, falling back to dynamic"
                    )
                    quantized = self._quantize_dynamic(model, backend)
                else:
                    quantized = self._quantize_static(model, calibration_data, backend)

            elif method == QuantizationMethod.QAT:
                logger.warning("QAT requires training, returning model with QAT config")
                quantized = self._prepare_qat(model, backend)

            elif method == QuantizationMethod.FP16:
                quantized = self._quantize_fp16(model)

            else:
                logger.info("   No quantization")
                return model

            # Calculate size reduction
            original_size = self._get_model_size(model)
            quantized_size = self._get_model_size(quantized)
            reduction = original_size / quantized_size if quantized_size > 0 else 1.0

            logger.info(f"   âœ… Quantization complete")
            logger.info(f"   Original size: {original_size:.1f} MB")
            logger.info(f"   Quantized size: {quantized_size:.1f} MB")
            logger.info(f"   Reduction: {reduction:.1f}x smaller")
            logger.info(f"   Expected speedup: 2-4x faster inference")

            return quantized

        except Exception as e:
            logger.error(f"Quantization failed: {e}")
            logger.warning("Returning original model")
            return model

    def _select_backend(self) -> QuantizationBackend:
        """Auto-select best quantization backend"""
        # Prefer FBGEMM on x86
        if QuantizationBackend.FBGEMM in self.available_backends:
            return QuantizationBackend.FBGEMM

        # QNNPACK on ARM
        if QuantizationBackend.QNNPACK in self.available_backends:
            return QuantizationBackend.QNNPACK

        # Fallback to PyTorch
        return QuantizationBackend.PYTORCH

    def _quantize_dynamic(self, model, backend: QuantizationBackend) -> Any:
        """
        Dynamic quantization (easiest, zero effort)

        Quantizes weights statically, activations dynamically at runtime
        Good for: RNNs, Transformers, models with variable input sizes
        Speedup: 2-3x on CPU
        """
        import torch
        from torch.quantization import quantize_dynamic

        logger.info("   Using dynamic quantization (zero effort!)")

        # Set backend
        if backend == QuantizationBackend.FBGEMM:
            torch.backends.quantized.engine = "fbgemm"
        elif backend == QuantizationBackend.QNNPACK:
            torch.backends.quantized.engine = "qnnpack"

        # Quantize
        quantized = quantize_dynamic(
            model,
            {torch.nn.Linear, torch.nn.LSTM, torch.nn.GRU},  # Quantize these layers
            dtype=torch.qint8,
        )

        return quantized

    def _quantize_static(
        self, model, calibration_data, backend: QuantizationBackend
    ) -> Any:
        """
        Static quantization (better accuracy than dynamic)

        Quantizes both weights and activations statically
        Requires: Calibration dataset
        Speedup: 3-4x on CPU
        """
        import torch
        from torch.quantization import get_default_qconfig, prepare, convert

        logger.info("   Using static quantization (with calibration)")

        # Set backend
        if backend == QuantizationBackend.FBGEMM:
            torch.backends.quantized.engine = "fbgemm"
            qconfig = torch.quantization.get_default_qconfig("fbgemm")
        elif backend == QuantizationBackend.QNNPACK:
            torch.backends.quantized.engine = "qnnpack"
            qconfig = torch.quantization.get_default_qconfig("qnnpack")
        else:
            qconfig = torch.quantization.default_qconfig

        # Prepare model
        model.eval()
        model.qconfig = qconfig

        # Fuse layers (important for accuracy!)
        # e.g., Conv + BN + ReLU â†’ single fused layer
        try:
            model = torch.quantization.fuse_modules(model, [["conv", "bn", "relu"]])
            logger.info("      Fused layers for better accuracy")
        except:
            pass

        # Prepare for calibration
        model_prepared = prepare(model)

        # Calibrate with representative data
        logger.info("      Calibrating with sample data...")
        model_prepared.eval()

        with torch.no_grad():
            for i, data in enumerate(calibration_data):
                if i >= 10:  # Use 10 batches
                    break
                _ = model_prepared(data)

        # Convert to quantized model
        quantized = convert(model_prepared)

        return quantized

    def _prepare_qat(self, model, backend: QuantizationBackend) -> Any:
        """
        Prepare model for Quantization-Aware Training

        Best accuracy, but requires retraining
        User needs to train this model
        """
        import torch
        from torch.quantization import get_default_qat_qconfig, prepare_qat

        logger.info("   Preparing for Quantization-Aware Training (QAT)")
        logger.info("      Note: You need to train this model!")

        # Set backend
        if backend == QuantizationBackend.FBGEMM:
            torch.backends.quantized.engine = "fbgemm"
            qconfig = torch.quantization.get_default_qat_qconfig("fbgemm")
        elif backend == QuantizationBackend.QNNPACK:
            torch.backends.quantized.engine = "qnnpack"
            qconfig = torch.quantization.get_default_qat_qconfig("qnnpack")
        else:
            qconfig = torch.quantization.default_qat_qconfig

        model.qconfig = qconfig

        # Fuse layers
        try:
            model = torch.quantization.fuse_modules(model, [["conv", "bn", "relu"]])
        except:
            pass

        # Prepare for QAT
        model_prepared = prepare_qat(model)

        logger.info("      Model ready for QAT training")
        logger.info("      Train normally, then call convert() to finalize")

        return model_prepared

    def _quantize_fp16(self, model) -> Any:
        """
        Convert model to FP16 (half precision)

        Simpler than INT8, still gives 2x memory savings
        Good for: GPUs with Tensor Cores
        """
        import torch

        logger.info("   Converting to FP16 (half precision)")

        model_fp16 = copy.deepcopy(model)
        model_fp16.half()

        return model_fp16

    def _get_model_size(self, model) -> float:
        """Get model size in MB"""
        import torch

        total_params = 0
        for param in model.parameters():
            total_params += param.numel() * param.element_size()

        for buffer in model.buffers():
            total_params += buffer.numel() * buffer.element_size()

        size_mb = total_params / (1024**2)
        return size_mb

    def compare_quantization_methods(
        self, model, example_input, calibration_data: Optional[Any] = None
    ) -> Dict:
        """
        Compare different quantization methods

        Args:
            model: PyTorch model
            example_input: Example input for testing
            calibration_data: Calibration data (for static quantization)

        Returns:
            Comparison results
        """
        import torch
        import time

        logger.info("ðŸ” Comparing quantization methods...")

        results = {}

        # Original model
        logger.info("   Testing original model...")
        original_size = self._get_model_size(model)
        original_time = self._benchmark_inference(model, example_input)

        results["original"] = {
            "size_mb": original_size,
            "inference_time_ms": original_time * 1000,
            "speedup": 1.0,
            "size_reduction": 1.0,
        }

        logger.info(
            f"      Size: {original_size:.1f} MB, Time: {original_time*1000:.2f}ms"
        )

        # Dynamic quantization
        logger.info("   Testing dynamic quantization...")
        try:
            dynamic_model = self.quantize(model, QuantizationMethod.DYNAMIC)
            dynamic_size = self._get_model_size(dynamic_model)
            dynamic_time = self._benchmark_inference(dynamic_model, example_input)

            results["dynamic"] = {
                "size_mb": dynamic_size,
                "inference_time_ms": dynamic_time * 1000,
                "speedup": original_time / dynamic_time,
                "size_reduction": original_size / dynamic_size,
            }

            logger.info(
                f"      Size: {dynamic_size:.1f} MB ({original_size/dynamic_size:.1f}x smaller)"
            )
            logger.info(
                f"      Time: {dynamic_time*1000:.2f}ms ({original_time/dynamic_time:.1f}x faster)"
            )

        except Exception as e:
            logger.warning(f"      Dynamic quantization failed: {e}")

        # Static quantization (if calibration data provided)
        if calibration_data is not None:
            logger.info("   Testing static quantization...")
            try:
                static_model = self.quantize(
                    model, QuantizationMethod.STATIC, calibration_data
                )
                static_size = self._get_model_size(static_model)
                static_time = self._benchmark_inference(static_model, example_input)

                results["static"] = {
                    "size_mb": static_size,
                    "inference_time_ms": static_time * 1000,
                    "speedup": original_time / static_time,
                    "size_reduction": original_size / static_size,
                }

                logger.info(
                    f"      Size: {static_size:.1f} MB ({original_size/static_size:.1f}x smaller)"
                )
                logger.info(
                    f"      Time: {static_time*1000:.2f}ms ({original_time/static_time:.1f}x faster)"
                )

            except Exception as e:
                logger.warning(f"      Static quantization failed: {e}")

        # FP16
        logger.info("   Testing FP16...")
        try:
            fp16_model = self.quantize(model, QuantizationMethod.FP16)
            fp16_size = self._get_model_size(fp16_model)

            # FP16 inference only works on GPU
            if torch.cuda.is_available():
                fp16_model = fp16_model.cuda()
                example_input_gpu = example_input.cuda().half()
                fp16_time = self._benchmark_inference(fp16_model, example_input_gpu)

                results["fp16"] = {
                    "size_mb": fp16_size,
                    "inference_time_ms": fp16_time * 1000,
                    "speedup": original_time / fp16_time,
                    "size_reduction": original_size / fp16_size,
                }

                logger.info(
                    f"      Size: {fp16_size:.1f} MB ({original_size/fp16_size:.1f}x smaller)"
                )
                logger.info(
                    f"      Time: {fp16_time*1000:.2f}ms ({original_time/fp16_time:.1f}x faster)"
                )
            else:
                results["fp16"] = {
                    "size_mb": fp16_size,
                    "size_reduction": original_size / fp16_size,
                }
                logger.info(
                    f"      Size: {fp16_size:.1f} MB ({original_size/fp16_size:.1f}x smaller)"
                )
                logger.info(f"      (Skipped timing - no GPU)")

        except Exception as e:
            logger.warning(f"      FP16 quantization failed: {e}")

        # Recommend best method
        best_method = self._recommend_method(results)
        logger.info(f"\nâœ… Recommended: {best_method}")

        return results

    def _benchmark_inference(self, model, example_input, num_runs: int = 100) -> float:
        """Benchmark model inference time"""
        import torch
        import time

        model.eval()

        # Warmup
        with torch.no_grad():
            for _ in range(10):
                _ = model(example_input)

        # Benchmark
        start_time = time.time()

        with torch.no_grad():
            for _ in range(num_runs):
                _ = model(example_input)

        total_time = time.time() - start_time
        avg_time = total_time / num_runs

        return avg_time

    def _recommend_method(self, results: Dict) -> str:
        """Recommend best quantization method"""
        # Prefer method with best speedup while keeping size reduction
        best_method = "original"
        best_score = 0

        for method, metrics in results.items():
            if method == "original":
                continue

            # Score = speedup * size_reduction
            speedup = metrics.get("speedup", 1.0)
            size_reduction = metrics.get("size_reduction", 1.0)
            score = speedup * size_reduction

            if score > best_score:
                best_score = score
                best_method = method

        return best_method


# Convenience functions
def quantize_model(
    model,
    method: QuantizationMethod = QuantizationMethod.DYNAMIC,
    calibration_data: Optional[Any] = None,
) -> Any:
    """
    Quantize model for 4x memory savings and 2-4x speedup

    Args:
        model: PyTorch model
        method: Quantization method
        calibration_data: Calibration data (for static quantization)

    Returns:
        Quantized model
    """
    quantizer = AutoQuantizer()
    return quantizer.quantize(model, method, calibration_data)


# Export
__all__ = [
    "QuantizationMethod",
    "QuantizationBackend",
    "QuantizationConfig",
    "AutoQuantizer",
    "quantize_model",
]
